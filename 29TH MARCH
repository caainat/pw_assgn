Q1. What is Lasso Regression, and how does it differ from other regression techniques?
Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that adds a penalty term to the OLS regression, 
specifically the absolute values of the coefficients. This penalty term helps shrink some coefficients to exactly zero, which can simplify the model.
Difference from other techniques: Unlike OLS or Ridge Regression, Lasso can effectively perform feature selection by shrinking irrelevant featur
e coefficients to zero, automatically removing them from the model.


Q2. What is the main advantage of using Lasso Regression in feature selection?
The main advantage is that Lasso automatically performs feature selection by shrinking some coefficients to zero. This helps eliminate irrelevant
features, making the model simpler and reducing overfitting.


Q3. How do you interpret the coefficients of a Lasso Regression model?
The coefficients show the strength and direction of the relationship between the independent variables and the target variable. However, since
Lasso can shrink coefficients to zero, if a coefficient is zero, it means that the corresponding feature has no effect on the model. Non-zero
coefficients represent the most important variables in predicting the outcome.


Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?
The key tuning parameter in Lasso is the regularization parameter (lambda).
Higher lambda: Increases the penalty, causing more coefficients to shrink to zero, which leads to a simpler model (fewer features).
Lower lambda: Reduces the penalty, making the model closer to an OLS regression with more non-zero coefficients.
Adjusting lambda balances model complexity (how many features are used) and model accuracy (fit to the data).


Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?
Lasso Regression itself is a linear model, but it can handle non-linear problems by using polynomial features or interactions between variables.
For example, you can transform the data (e.g., add squared or interaction terms), then apply Lasso Regression to this expanded feature set to 
capture non-linear relationships.


Q6. What is the difference between Ridge Regression and Lasso Regression?
The main difference is in how they apply regularization:
Ridge Regression: Adds a penalty on the squared coefficients, which shrinks the coefficients but does not set them to zero.
Lasso Regression: Adds a penalty on the absolute values of the coefficients, which can shrink some coefficients to zero, allowing for feature selection.
Ridge is useful when all features are important but we want to control overfitting, while Lasso is better when we suspect many features are irrelevant.


Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?
Yes, Lasso can handle multicollinearity (high correlation between features) by selecting one variable from a group of highly correlated variables and 
shrinking the others to zero. This way, it helps reduce redundancy in the model, unlike Ridge Regression, which retains all correlated features but shrinks
their coefficients.


Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?
The optimal value of lambda is typically chosen using cross-validation. In cross-validation, different values of lambda are tested on subsets of the data, and the value that gives the best predictive performance on unseen data is selected.
